# transformer from scratch!
The aim of this project is to understand the *Attention* mechanism from this [paper](https://arxiv.org/abs/1706.03762), especially the decoder block (taken from GPT2 architecture). The goal is to predict the next word given the context of the word. For example, given the sentence "marry want a little lamb", we want to know what comes after. In this project the GPT2 tokenizer will be used (BPE), although it is also possible to create your own tokenizer.

In this repo we are using a relatively small model with only ~6M parameters, but since the goal is to understand the Attention mechanism, it should be suffice enough.

*Note: this is an on going repo, meaning new things will be added, such as RoPE embedding, GQA, etc..*

# dataset
In this project we are going to use text data from Shakespeare's novel. With the objective to produce a shakespearean texts.

# results
![train-val-figure](https://github.com/user-attachments/assets/c764be3d-9cdf-4e4a-abc8-c09b2e849998)

Based on the train and val loss, it can  be seen that the model overfits quite a bit, and seeing the value of loss, the model also doesn't have a really good performance. This might be because how small the dataset that we are using and how small the training was conducted.

Below are the text that have been generated by the model:
```
Oh, dear a man,
The emptier than he is but a gentleman
That you may not. This is no; I'll
your honour.

BRUTUS:
Let's not to the ice,
And, to hear me, the people'st of
s of your brows.

BRUTUS:
No, sir, you are deceived, that he is not,
When it is not.

MENENIUS:
Let them, a
shoulder-harming: what, the
Of the
Of an enemy, a man, I'll plead it not
him.

BRUTUS:
If the gods
By his burning?

CORIOLAN
```

Pretty good for a 6M parameters model, althought it seems BRUTUS is talking to himself. Other than that it looks like the text is coherent as well. 
