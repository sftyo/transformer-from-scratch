{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as  plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 5\n",
    "d_model = 128\n",
    "x_m = torch.randn((n_tokens, d_model), dtype = torch.float32)\n",
    "x_n = torch.randn((n_tokens, d_model), dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_q = nn.Linear(d_model, d_model, bias = False)\n",
    "W_k = nn.Linear(d_model, d_model, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_d_model = d_model // 2\n",
    "freq_range = torch.arange(half_d_model, dtype=torch.float32) \n",
    "freq_rates = torch.pow(10000, -2* freq_range / d_model) # theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = W_q(x_m).unsqueeze(0) # (B, N , D_MODEL)\n",
    "k = W_k(x_n).unsqueeze(0)\n",
    "bsz, seq_len, dim = q.shape\n",
    "\n",
    "pos = torch.arange(seq_len, dtype = torch.float32)\n",
    "freqs = torch.einsum('i,j->ij', pos, freq_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_embd = torch.sin(freqs)\n",
    "cos_embd = torch.sin(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 128])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_sin, q_cos = q[..., ::2] , q[..., 1::2] # even and odd pos\n",
    "k_sin, k_cos = k[..., ::2], k[..., 1::2]\n",
    "\n",
    "\n",
    "\n",
    "q_rot = torch.cat([q_sin * cos_embd.unsqueeze(0) - q_cos * sin_embd.unsqueeze(0),\n",
    "           q_sin * sin_embd.unsqueeze(0) - q_cos * cos_embd.unsqueeze(0)],\n",
    "          dim = -1)\n",
    "\n",
    "q_rot.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class RoPE:\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.freqs = self._build_frequencies(dim)\n",
    "\n",
    "    def _build_frequencies(self, dim):\n",
    "        \"\"\"\n",
    "        Create the frequency matrix for RoPE.\n",
    "        The frequencies are generated in a geometric sequence.\n",
    "        \"\"\"\n",
    "        half_dim = dim // 2\n",
    "        freq_range = torch.arange(half_dim, dtype=torch.float32)\n",
    "        freq_rates = torch.pow(10000, -2 * freq_range / dim)\n",
    "        return freq_rates\n",
    "\n",
    "    def apply(self, q, k):\n",
    "        \"\"\"\n",
    "        Apply RoPE to the given query and key tensors.\n",
    "        \n",
    "        Parameters:\n",
    "            q: (batch_size, num_heads, seq_len, head_dim)\n",
    "            k: (batch_size, num_heads, seq_len, head_dim)\n",
    "            \n",
    "        Returns:\n",
    "            q_rot, k_rot: RoPE applied query and key tensors\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, seq_len, head_dim = q.shape\n",
    "        half_head_dim = head_dim // 2  # RoPE operates on even/odd splits of the dimension\n",
    "\n",
    "        # Position ids: Shape (seq_len, 1)\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.float32, device=q.device).unsqueeze(1)\n",
    "\n",
    "        # Calculate sin and cos embeddings based on position ids and frequency rates\n",
    "        theta = torch.einsum('i,j->ij', position_ids.squeeze(-1), self.freqs.to(q.device))  # (seq_len, half_head_dim)\n",
    "        sin_embed = torch.sin(theta).unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, half_head_dim)\n",
    "        cos_embed = torch.cos(theta).unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, half_head_dim)\n",
    "\n",
    "        # Split q and k into even (first half) and odd (second half) dimensions\n",
    "        q1, q2 = q[..., :half_head_dim], q[..., half_head_dim:]\n",
    "        k1, k2 = k[..., :half_head_dim], k[..., half_head_dim:]\n",
    "\n",
    "        # Apply the rotational position encoding\n",
    "        q_rot = torch.cat([q1 * cos_embed - q2 * sin_embed, q1 * sin_embed + q2 * cos_embed], dim=-1)\n",
    "        k_rot = torch.cat([k1 * cos_embed - k2 * sin_embed, k1 * sin_embed + k2 * cos_embed], dim=-1)\n",
    "\n",
    "        return q_rot, k_rot\n",
    "\n",
    "# Example usage\n",
    "seq_len = 128\n",
    "head_dim = 64\n",
    "batch_size = 32\n",
    "num_heads = 8\n",
    "\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "rope = RoPE(head_dim)\n",
    "q_rot, k_rot = rope.apply(q, k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
